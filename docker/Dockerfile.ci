ARG VLLM_BASE_IMAGE=vllm/vllm-openai
ARG VLLM_BASE_TAG=v0.15.0
FROM ${VLLM_BASE_IMAGE}:${VLLM_BASE_TAG}
ARG APP_DIR=/workspace/vllm-omni
WORKDIR ${APP_DIR}

COPY . .

# Install system dependencies
RUN apt-get update && \
    apt-get install -y ffmpeg sox libsox-fmt-all && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Install vLLM wheel for the specific commit
# URL format: https://wheels.vllm.ai/<commit_hash>/
# Current vLLM commit: f97ca671766c5201404e9fc812e35bf2c4e95a01
# Note: --prerelease=allow is required because nightly/commit wheels use
# pre-release version numbers (e.g. 2.10.0.dev...) which uv ignores by default,
# causing it to fall back to the older stable release from PyPI.
RUN uv pip install --system --upgrade --force-reinstall --prerelease=allow vllm --extra-index-url https://wheels.vllm.ai/4a1550d22d7058e129d0e1257e726b3bf4a77025

# Keep flashinfer runtime packages in version sync.
# We derive the version from installed flashinfer-python (pulled by vllm)
# and enforce the same version for flashinfer-cubin / flashinfer-jit-cache.
# RUN FLASHINFER_VERSION="$(python3 -c 'import importlib.metadata as m; print(m.version("flashinfer-python"))')" && \
#     FLASHINFER_CUDA_TAG="$(python3 -c 'import torch; print((torch.version.cuda or "12.4").replace(".", ""))')" && \
#     uv pip install --system --upgrade --force-reinstall \
#       "flashinfer-cubin==${FLASHINFER_VERSION}" \
#       "flashinfer-jit-cache==${FLASHINFER_VERSION}" \
#       --extra-index-url "https://flashinfer.ai/whl/cu${FLASHINFER_CUDA_TAG}" && \
#     flashinfer show-config

# Install vllm-omni into the same uv-managed Python environment used by the base image.
RUN uv pip install --python "$(python3 -c 'import sys; print(sys.executable)')" --no-cache-dir ".[dev]"

RUN ln -sf /usr/bin/python3 /usr/bin/python

ENTRYPOINT []
